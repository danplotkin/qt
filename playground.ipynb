{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## qt Training Playground"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_from_disk\n",
    "from utils.torch_datasets import MiniPileDataset\n",
    "from utils.tokenizer import get_tokenizer\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "from tqdm import tqdm\n",
    "import torch\n",
    "\n",
    "\n",
    "bestof = torch.load('data/flattened_corpa/reddit_comments/bestof.pt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "257059775"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(bestof)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_ds = MiniPileDataset(path='data/flattened_corpa/minipile/train.pt', block_size=100)\n",
    "train_loader = DataLoader(train_ds, batch_size=32, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([   39,  4825,   338, 29237,  1041, 23492,   318,  1695,   284,   662,\n",
       "            12,  2875,   329,   720, 45455,   198,   198,  1135,  1053,  1775,\n",
       "          6088,   286, 40210,    12, 18143,   509,  4663, 42388,   287,   674,\n",
       "           640,    11,   617,  1365,   621,  1854,    13, 20463,    11,  2158,\n",
       "            11,   711,  2407,   523,  3264,   319,   262,  1438,   355,  6913,\n",
       "         21206,   338,  1355,  1039,    13,  1114,   720,  1495,    11,  4344,\n",
       "          1010,   651,   257,   900,   286, 22537,   326,  6842,  1310,  1277,\n",
       "         28204,   284,  1583,    13, 30882,   338,  6597,  7733,   286,  3572,\n",
       "            11,   475,   389,   645,  4719,  5421,   284, 14947,  2460,  1377,\n",
       "           379,  1551,    11,   510,  1566,   484,   766,   257,  6808, 20236]),\n",
       " tensor([ 4825,   338, 29237,  1041, 23492,   318,  1695,   284,   662,    12,\n",
       "          2875,   329,   720, 45455,   198,   198,  1135,  1053,  1775,  6088,\n",
       "           286, 40210,    12, 18143,   509,  4663, 42388,   287,   674,   640,\n",
       "            11,   617,  1365,   621,  1854,    13, 20463,    11,  2158,    11,\n",
       "           711,  2407,   523,  3264,   319,   262,  1438,   355,  6913, 21206,\n",
       "           338,  1355,  1039,    13,  1114,   720,  1495,    11,  4344,  1010,\n",
       "           651,   257,   900,   286, 22537,   326,  6842,  1310,  1277, 28204,\n",
       "           284,  1583,    13, 30882,   338,  6597,  7733,   286,  3572,    11,\n",
       "           475,   389,   645,  4719,  5421,   284, 14947,  2460,  1377,   379,\n",
       "          1551,    11,   510,  1566,   484,   766,   257,  6808, 20236, 11112]))"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_ds[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input Ids:\n",
      "HTC's Vive Pro headset is available to pre-order for $799\n",
      "\n",
      "We've seen plenty of Beats-focused KIRFs in our time, some better than others. Few, however, play quite so directly on the name as OrigAudio's Beets. For $25, adopters get a set of headphones that bear little direct resemblance to Dr. Dre's audio gear of choice, but are no doubt bound to impress friends -- at least, up until they see a root vegetable\n",
      "\n",
      "Labels:\n",
      "TC's Vive Pro headset is available to pre-order for $799\n",
      "\n",
      "We've seen plenty of Beats-focused KIRFs in our time, some better than others. Few, however, play quite so directly on the name as OrigAudio's Beets. For $25, adopters get a set of headphones that bear little direct resemblance to Dr. Dre's audio gear of choice, but are no doubt bound to impress friends -- at least, up until they see a root vegetable logo\n",
      "\n",
      "\n",
      "\n",
      "Input Ids:\n",
      " logo instead of a lower-case B. Thankfully, there's more to it than just amusing and confusing peers. Every purchase will lead to a donation of canned beets (what else?) to the Second Harvest Food Bank of Orange County. For us, that's reason enough to hope that Beats doesn't put the kibosh on OrigAudio's effort. Besides, we could use some accompaniment for our BeetBox.Q:\n",
      "\n",
      "NullPointerException in getview of custom adapter\n",
      "\n",
      "\n",
      "Labels:\n",
      " instead of a lower-case B. Thankfully, there's more to it than just amusing and confusing peers. Every purchase will lead to a donation of canned beets (what else?) to the Second Harvest Food Bank of Orange County. For us, that's reason enough to hope that Beats doesn't put the kibosh on OrigAudio's effort. Besides, we could use some accompaniment for our BeetBox.Q:\n",
      "\n",
      "NullPointerException in getview of custom adapter\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Input Ids:\n",
      "\n",
      "I'm getting image from bitmap method and trying to populate the listview. But when i call the bitmap function inside getview the nullpointerException error occurs. please help me... \n",
      "here is my view Activity class:\n",
      "public class Viewactivity extends Activity{\n",
      "\n",
      "    TextView tv;\n",
      "    ImageView im;\n",
      "\n",
      "    @Override\n",
      "    protected void onCreate(Bundle savedInstanceState) {\n",
      "     \n",
      "\n",
      "Labels:\n",
      "I'm getting image from bitmap method and trying to populate the listview. But when i call the bitmap function inside getview the nullpointerException error occurs. please help me... \n",
      "here is my view Activity class:\n",
      "public class Viewactivity extends Activity{\n",
      "\n",
      "    TextView tv;\n",
      "    ImageView im;\n",
      "\n",
      "    @Override\n",
      "    protected void onCreate(Bundle savedInstanceState) {\n",
      "      \n",
      "\n",
      "\n",
      "\n",
      "Input Ids:\n",
      "   super.onCreate(savedInstanceState);\n",
      "        setContentView(R.layout.views);\n",
      "\n",
      "        ListView mListView = (ListView)findViewById(R.id.listView);\n",
      "        //array houlds all images\n",
      "        int Images[] = new int[]{\n",
      "         \n",
      "\n",
      "Labels:\n",
      "  super.onCreate(savedInstanceState);\n",
      "        setContentView(R.layout.views);\n",
      "\n",
      "        ListView mListView = (ListView)findViewById(R.id.listView);\n",
      "        //array houlds all images\n",
      "        int Images[] = new int[]{\n",
      "          \n",
      "\n",
      "\n",
      "\n",
      "Input Ids:\n",
      "   R.drawable.confidential,\n",
      "            ...     \n",
      "            };\n",
      "        //array holds all strings to be drawn in the image\n",
      "\n",
      "        CustomList adaptor = new CustomList(this , Images);\n",
      "        mListView.setAdapter(adaptor);\n",
      "\n",
      "\n",
      "\n",
      "Labels:\n",
      "  R.drawable.confidential,\n",
      "            ...     \n",
      "            };\n",
      "        //array holds all strings to be drawn in the image\n",
      "\n",
      "        CustomList adaptor = new CustomList(this , Images);\n",
      "        mListView.setAdapter(adaptor);\n",
      "\n",
      " \n",
      "\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for i in range(5):\n",
    "    batch = train[i]\n",
    "    input_ids = tokenizer.decode(batch[\"input_ids\"])\n",
    "    labels = tokenizer.decode(batch[\"labels\"])\n",
    "    print(f'Input Ids:\\n{input_ids}\\n')\n",
    "    print(f\"Labels:\\n{labels}\")\n",
    "    print('\\n\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Estimating Parameters based on Architecture Hyperparameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "------------Settings-----------------\n",
      "d_model: 2048\n",
      "d_ff: 8192\n",
      "num_layers: 1\n",
      "vocab_size: 50300\n",
      "seq_len: 2048\n",
      "batch_size: 64\n",
      "assumes fp32 params\n",
      "------------Parameters---------------\n",
      "params per decoder layer: 67,108,864\n",
      "--------------------------------------\n",
      "total nonembedding params: 67,108,864\n",
      "total embedding params: 103,014,400\n",
      "--------------------------------------\n",
      "\n",
      "total params: 170,123,264\n",
      "\n",
      "----------------Memory----------------\n",
      "\n",
      "memory footprint of model during training: 3 GBs\n",
      "memory footprint of model during inference: 0.0 GBs\n",
      "\n",
      "--------------------------------------\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# settings\n",
    "d_model = 2048\n",
    "num_heads = 16\n",
    "assert d_model % num_heads == 0\n",
    "d_ff = 4*d_model\n",
    "num_layers = 14\n",
    "# num_layers = 1 # NOTE for testing\n",
    "vocab_size = 50300\n",
    "\n",
    "seq_len = 2048\n",
    "batch_size = 64\n",
    "\n",
    "\n",
    "## parameter calcs\n",
    "decoder_params = 2*4*d_model**2 + 2*d_model*d_ff\n",
    "\n",
    "embedding_params = vocab_size * d_model\n",
    "nonembedding_params = decoder_params * num_layers\n",
    "\n",
    "total_params = embedding_params + nonembedding_params\n",
    "\n",
    "## memory footprint calcs\n",
    "\n",
    "model_footprint = 4 * total_params\n",
    "\n",
    "# inference footprint in GBs\n",
    "inference_footprint = 1.2 * model_footprint // 10**9\n",
    "\n",
    "adam_footprint = 12 * total_params\n",
    "gradients_footprint = 4 * total_params\n",
    "activations_footprint = 2*seq_len*batch_size*d_model*num_layers\n",
    "\n",
    "# training footprint in GBs\n",
    "training_footprint = (model_footprint + \\\n",
    "    adam_footprint + \\\n",
    "    gradients_footprint + \\\n",
    "    activations_footprint) // 10**9\n",
    "\n",
    "model_card_str = f'''\n",
    "------------Settings-----------------\n",
    "d_model: {d_model}\n",
    "d_ff: {d_ff}\n",
    "num_layers: {num_layers}\n",
    "vocab_size: {vocab_size}\n",
    "seq_len: {seq_len}\n",
    "batch_size: {batch_size}\n",
    "assumes fp32 params\n",
    "------------Parameters---------------\n",
    "params per decoder layer: {decoder_params:,}\n",
    "--------------------------------------\n",
    "total nonembedding params: {nonembedding_params:,}\n",
    "total embedding params: {embedding_params:,}\n",
    "--------------------------------------\n",
    "\n",
    "total params: {total_params:,}\n",
    "\n",
    "----------------Memory----------------\n",
    "\n",
    "memory footprint of model during training: {training_footprint} GBs\n",
    "memory footprint of model during inference: {inference_footprint} GBs\n",
    "\n",
    "--------------------------------------\n",
    "\n",
    "'''\n",
    "\n",
    "print(model_card_str)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Trial Training Run"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/48 [00:00<?, ?it/s]\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "type object 'GPT2TokenizerFast' has no attribute 'pad_token_id'",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mAttributeError\u001b[39m                            Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[1]\u001b[39m\u001b[32m, line 63\u001b[39m\n\u001b[32m     52\u001b[39m \u001b[38;5;66;03m# ## pretrain\u001b[39;00m\n\u001b[32m     53\u001b[39m trainer = Trainer(\n\u001b[32m     54\u001b[39m     model = qt,\n\u001b[32m     55\u001b[39m     train_loader=train_loader,\n\u001b[32m   (...)\u001b[39m\u001b[32m     60\u001b[39m     device=device\n\u001b[32m     61\u001b[39m )\n\u001b[32m---> \u001b[39m\u001b[32m63\u001b[39m \u001b[43mtrainer\u001b[49m\u001b[43m.\u001b[49m\u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/vault/gradschool/qt/utils/training.py:113\u001b[39m, in \u001b[36mTrainer.train\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m    111\u001b[39m inputs, targets = inputs.to(\u001b[38;5;28mself\u001b[39m.device), targets.to(\u001b[38;5;28mself\u001b[39m.device)\n\u001b[32m    112\u001b[39m \u001b[38;5;28mself\u001b[39m.optimizer.zero_grad()\n\u001b[32m--> \u001b[39m\u001b[32m113\u001b[39m logits = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\u001b[43minputs\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;66;03m# batch, seq_len, vocab_size\u001b[39;00m\n\u001b[32m    114\u001b[39m \u001b[38;5;66;03m# Shift for next-token prediction\u001b[39;00m\n\u001b[32m    115\u001b[39m loss = \u001b[38;5;28mself\u001b[39m.criterion(logits, targets)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/vault/gradschool/dlvenv/lib/python3.11/site-packages/torch/nn/modules/module.py:1739\u001b[39m, in \u001b[36mModule._wrapped_call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1737\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._compiled_call_impl(*args, **kwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[32m   1738\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1739\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/vault/gradschool/dlvenv/lib/python3.11/site-packages/torch/nn/modules/module.py:1750\u001b[39m, in \u001b[36mModule._call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1745\u001b[39m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[32m   1746\u001b[39m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[32m   1747\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m._backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_pre_hooks\n\u001b[32m   1748\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[32m   1749\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[32m-> \u001b[39m\u001b[32m1750\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1752\u001b[39m result = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1753\u001b[39m called_always_called_hooks = \u001b[38;5;28mset\u001b[39m()\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/vault/gradschool/qt/utils/transformer/model.py:83\u001b[39m, in \u001b[36mQT.forward\u001b[39m\u001b[34m(self, tgt)\u001b[39m\n\u001b[32m     81\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, tgt: torch.Tensor) -> torch.Tensor:\n\u001b[32m     82\u001b[39m \u001b[38;5;250m    \u001b[39m\u001b[33;03m\"\"\"Decoder-only transformer forward pass\"\"\"\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m83\u001b[39m     tgt_mask = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mgenerate_mask\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtgt\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     84\u001b[39m     \u001b[38;5;66;03m# tgt_embedded = self.dropout(self.positional_encoding(self.decoder_embedding(tgt)))\u001b[39;00m\n\u001b[32m     85\u001b[39m     \u001b[38;5;66;03m# NOTE no positional encodings, using ALiBi \u001b[39;00m\n\u001b[32m     86\u001b[39m     tgt_embedded = \u001b[38;5;28mself\u001b[39m.dropout(\u001b[38;5;28mself\u001b[39m.decoder_embedding(tgt))\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/vault/gradschool/qt/utils/transformer/model.py:75\u001b[39m, in \u001b[36mQT.generate_mask\u001b[39m\u001b[34m(self, tgt)\u001b[39m\n\u001b[32m     73\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mgenerate_mask\u001b[39m(\u001b[38;5;28mself\u001b[39m, tgt: torch.Tensor) -> torch.Tensor:\n\u001b[32m     74\u001b[39m \u001b[38;5;250m    \u001b[39m\u001b[33;03m\"\"\"Generate casual and padding mask\"\"\"\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m75\u001b[39m     tgt_mask = (tgt != \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mtokenizer\u001b[49m\u001b[43m.\u001b[49m\u001b[43mpad_token_id\u001b[49m).unsqueeze(\u001b[32m1\u001b[39m).unsqueeze(\u001b[32m3\u001b[39m)\n\u001b[32m     76\u001b[39m     seq_length = tgt.size(\u001b[32m1\u001b[39m)\n\u001b[32m     77\u001b[39m     nopeak_mask = (\u001b[32m1\u001b[39m - torch.triu(torch.ones(\u001b[32m1\u001b[39m, seq_length, seq_length), diagonal=\u001b[32m1\u001b[39m)).bool().to(tgt.device)\n",
      "\u001b[31mAttributeError\u001b[39m: type object 'GPT2TokenizerFast' has no attribute 'pad_token_id'"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import logging\n",
    "import torch\n",
    "from torchinfo import summary # TODO add this to requirements.txt\n",
    "from torcheval.metrics.text import Perplexity\n",
    "from utils.configs import load_configs\n",
    "from transformers import GPT2TokenizerFast\n",
    "from utils.torch_datasets import MiniPileDataset\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "\n",
    "from utils.transformer.model import QT\n",
    "from utils.training import Trainer\n",
    "\n",
    "## load configs, logger, and device\n",
    "config = load_configs()\n",
    "# logs saves to training.log in harm2d directory\n",
    "logger = logging.getLogger(__name__)\n",
    "logging.basicConfig(\n",
    "    filename=config['training'].logging_dir,\n",
    "    filemode='w',\n",
    "    level=logging.DEBUG,\n",
    "    format='%(asctime)s - %(levelname)s - %(message)s'\n",
    ")\n",
    "# get device\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "## get datasets\n",
    "train = MiniPileDataset(\n",
    "    path='data/tokenized/validation_tokens.pt', \n",
    "    block_size=config['transformer'].max_seq_length\n",
    ")\n",
    "valid = MiniPileDataset(\n",
    "    path='data/tokenized/test_tokens.pt', \n",
    "    block_size=config['transformer'].max_seq_length\n",
    ")\n",
    "\n",
    "## get dataloaders\n",
    "train_loader = DataLoader(train, batch_size=config['training'].batch_size, shuffle=True)\n",
    "valid_loader = DataLoader(valid, batch_size=config['training'].batch_size, shuffle=False)\n",
    "\n",
    "## get model\n",
    "qt = QT(\n",
    "    config=config['transformer'],\n",
    "    tokenizer = GPT2TokenizerFast,\n",
    "    device = device \n",
    ")\n",
    "\n",
    "model_card_str = summary(qt)\n",
    "logging.info('\\n' + str(model_card_str))\n",
    "logging.info(config)\n",
    "\n",
    "# ## pretrain\n",
    "trainer = Trainer(\n",
    "    model = qt,\n",
    "    train_loader=train_loader,\n",
    "    val_loader=valid_loader,\n",
    "    config=config['training'],\n",
    "    criterion = torch.nn.CrossEntropyLoss(),\n",
    "    metric = Perplexity(),\n",
    "    device=device\n",
    ")\n",
    "\n",
    "trainer.train()\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
