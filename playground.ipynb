{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## qt Training Playground"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_from_disk\n",
    "from utils.torch_datasets import MiniPileDataset\n",
    "from utils.tokenizer import get_tokenizer\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "from tqdm import tqdm\n",
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'input_ids': [39,\n",
       "  4825,\n",
       "  338,\n",
       "  29237,\n",
       "  1041,\n",
       "  23492,\n",
       "  318,\n",
       "  1695,\n",
       "  284,\n",
       "  662,\n",
       "  12,\n",
       "  2875,\n",
       "  329,\n",
       "  720,\n",
       "  45455,\n",
       "  198,\n",
       "  198,\n",
       "  1135,\n",
       "  1053,\n",
       "  1775,\n",
       "  6088,\n",
       "  286,\n",
       "  40210,\n",
       "  12,\n",
       "  18143,\n",
       "  509,\n",
       "  4663,\n",
       "  42388,\n",
       "  287,\n",
       "  674,\n",
       "  640,\n",
       "  11,\n",
       "  617,\n",
       "  1365,\n",
       "  621,\n",
       "  1854,\n",
       "  13,\n",
       "  20463,\n",
       "  11,\n",
       "  2158,\n",
       "  11,\n",
       "  711,\n",
       "  2407,\n",
       "  523,\n",
       "  3264,\n",
       "  319,\n",
       "  262,\n",
       "  1438,\n",
       "  355,\n",
       "  6913,\n",
       "  21206,\n",
       "  338,\n",
       "  1355,\n",
       "  1039,\n",
       "  13,\n",
       "  1114,\n",
       "  720,\n",
       "  1495,\n",
       "  11,\n",
       "  4344,\n",
       "  1010,\n",
       "  651,\n",
       "  257,\n",
       "  900,\n",
       "  286,\n",
       "  22537,\n",
       "  326,\n",
       "  6842,\n",
       "  1310,\n",
       "  1277,\n",
       "  28204,\n",
       "  284,\n",
       "  1583,\n",
       "  13,\n",
       "  30882,\n",
       "  338,\n",
       "  6597,\n",
       "  7733,\n",
       "  286,\n",
       "  3572,\n",
       "  11,\n",
       "  475,\n",
       "  389,\n",
       "  645,\n",
       "  4719,\n",
       "  5421,\n",
       "  284,\n",
       "  14947,\n",
       "  2460,\n",
       "  1377,\n",
       "  379,\n",
       "  1551,\n",
       "  11,\n",
       "  510,\n",
       "  1566,\n",
       "  484,\n",
       "  766,\n",
       "  257,\n",
       "  6808,\n",
       "  20236,\n",
       "  11112,\n",
       "  2427,\n",
       "  286,\n",
       "  257,\n",
       "  2793,\n",
       "  12,\n",
       "  7442,\n",
       "  347,\n",
       "  13,\n",
       "  31251,\n",
       "  11,\n",
       "  612,\n",
       "  338,\n",
       "  517,\n",
       "  284,\n",
       "  340,\n",
       "  621,\n",
       "  655,\n",
       "  28297,\n",
       "  290,\n",
       "  15337,\n",
       "  14495,\n",
       "  13,\n",
       "  3887,\n",
       "  5001,\n",
       "  481,\n",
       "  1085,\n",
       "  284,\n",
       "  257,\n",
       "  13784,\n",
       "  286,\n",
       "  32530,\n",
       "  307,\n",
       "  1039,\n",
       "  357,\n",
       "  10919,\n",
       "  2073,\n",
       "  10091,\n",
       "  284,\n",
       "  262,\n",
       "  5498,\n",
       "  26149,\n",
       "  7318,\n",
       "  5018,\n",
       "  286,\n",
       "  11942,\n",
       "  3418,\n",
       "  13,\n",
       "  1114,\n",
       "  514,\n",
       "  11,\n",
       "  326,\n",
       "  338,\n",
       "  1738,\n",
       "  1576,\n",
       "  284,\n",
       "  2911,\n",
       "  326,\n",
       "  40210,\n",
       "  1595,\n",
       "  470,\n",
       "  1234,\n",
       "  262,\n",
       "  479,\n",
       "  571,\n",
       "  3768,\n",
       "  319,\n",
       "  6913,\n",
       "  21206,\n",
       "  338,\n",
       "  3626,\n",
       "  13,\n",
       "  16238,\n",
       "  11,\n",
       "  356,\n",
       "  714,\n",
       "  779,\n",
       "  617,\n",
       "  10600,\n",
       "  3681,\n",
       "  329,\n",
       "  674,\n",
       "  39040,\n",
       "  14253,\n",
       "  13]}"
      ]
     },
     "execution_count": 80,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "hugging_face_datadict = load_from_disk('data/tokenized/minipile')\n",
    "hugging_face_datadict['train'][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SmallMiniPileDataset(Dataset):\n",
    "    def __init__(self, corpa: list, block_size: int, stride: int = None, offset: int = 0):\n",
    "        self.tokens = torch.tensor(corpa)\n",
    "        self.offset = offset\n",
    "        self.block_size = block_size\n",
    "        self.stride = stride if stride is not None else block_size\n",
    "        # self.indices = list(range(0, len(self.tokens) - block_size, self.stride))\n",
    "        self.indices = list(range(0 + offset, len(self.tokens) - block_size, self.stride))\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.indices)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        start = self.indices[idx]\n",
    "        chunk = self.tokens[start : start + self.block_size + 1]\n",
    "        input_tensor = chunk[:-1].clone().long()\n",
    "        label_tensor = chunk[1:].clone().long()\n",
    "        return {\"input_ids\": input_tensor, \"labels\": label_tensor}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "corpa = [1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'input_ids': tensor([6, 7, 8]), 'labels': tensor([7, 8, 9])}"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ds = SmallMiniPileDataset(corpa=corpa, block_size=3, offset=2)\n",
    "ds[-1]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2, 5]\n"
     ]
    }
   ],
   "source": [
    "print(ds.indices)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from datasets import load_from_disk\n",
    "# from tqdm import tqdm\n",
    "# import torch\n",
    "\n",
    "# ds = load_from_disk(\"data/tokenized/minipile\")\n",
    "# ds\n",
    "# # all_ids = []\n",
    "# # for example in tqdm(ds['train']):\n",
    "# #     all_ids.extend(example[\"input_ids\"])\n",
    "\n",
    "# # torch.save(torch.tensor(all_ids, dtype=torch.long), \"data/tokenized/flattened_ids.pt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "train = MiniPileDataset(path='data/tokenized/flattened_ids.pt', block_size=20)\n",
    "tokenizer = get_tokenizer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_loader = DataLoader(train, batch_size=32, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 9683/2649621 [00:10<48:00, 916.46it/s]  \n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mKeyboardInterrupt\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[73]\u001b[39m\u001b[32m, line 3\u001b[39m\n\u001b[32m      1\u001b[39m pbar = tqdm(\u001b[38;5;28menumerate\u001b[39m(train_loader), total=\u001b[38;5;28mlen\u001b[39m(train_loader))\n\u001b[32m----> \u001b[39m\u001b[32m3\u001b[39m \u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mi\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbatch\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mpbar\u001b[49m\u001b[43m:\u001b[49m\n\u001b[32m      4\u001b[39m \u001b[43m    \u001b[49m\u001b[43mtorch\u001b[49m\u001b[43m.\u001b[49m\u001b[43msum\u001b[49m\u001b[43m(\u001b[49m\u001b[43mbatch\u001b[49m\u001b[43m[\u001b[49m\u001b[33;43m'\u001b[39;49m\u001b[33;43minput_ids\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/MSAI/courses/spring_2025/nlp/qt/venv/lib/python3.12/site-packages/tqdm/std.py:1181\u001b[39m, in \u001b[36mtqdm.__iter__\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m   1178\u001b[39m time = \u001b[38;5;28mself\u001b[39m._time\n\u001b[32m   1180\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1181\u001b[39m \u001b[43m    \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mobj\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43miterable\u001b[49m\u001b[43m:\u001b[49m\n\u001b[32m   1182\u001b[39m \u001b[43m        \u001b[49m\u001b[38;5;28;43;01myield\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mobj\u001b[49m\n\u001b[32m   1183\u001b[39m \u001b[43m        \u001b[49m\u001b[38;5;66;43;03m# Update and possibly print the progressbar.\u001b[39;49;00m\n\u001b[32m   1184\u001b[39m \u001b[43m        \u001b[49m\u001b[38;5;66;43;03m# Note: does not call self.update(1) for speed optimisation.\u001b[39;49;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/MSAI/courses/spring_2025/nlp/qt/venv/lib/python3.12/site-packages/torch/utils/data/dataloader.py:733\u001b[39m, in \u001b[36m_BaseDataLoaderIter.__next__\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m    730\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m._sampler_iter \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m    731\u001b[39m     \u001b[38;5;66;03m# TODO(https://github.com/pytorch/pytorch/issues/76750)\u001b[39;00m\n\u001b[32m    732\u001b[39m     \u001b[38;5;28mself\u001b[39m._reset()  \u001b[38;5;66;03m# type: ignore[call-arg]\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m733\u001b[39m data = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_next_data\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    734\u001b[39m \u001b[38;5;28mself\u001b[39m._num_yielded += \u001b[32m1\u001b[39m\n\u001b[32m    735\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m (\n\u001b[32m    736\u001b[39m     \u001b[38;5;28mself\u001b[39m._dataset_kind == _DatasetKind.Iterable\n\u001b[32m    737\u001b[39m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m._IterableDataset_len_called \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m    738\u001b[39m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m._num_yielded > \u001b[38;5;28mself\u001b[39m._IterableDataset_len_called\n\u001b[32m    739\u001b[39m ):\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/MSAI/courses/spring_2025/nlp/qt/venv/lib/python3.12/site-packages/torch/utils/data/dataloader.py:789\u001b[39m, in \u001b[36m_SingleProcessDataLoaderIter._next_data\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m    787\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34m_next_data\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[32m    788\u001b[39m     index = \u001b[38;5;28mself\u001b[39m._next_index()  \u001b[38;5;66;03m# may raise StopIteration\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m789\u001b[39m     data = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_dataset_fetcher\u001b[49m\u001b[43m.\u001b[49m\u001b[43mfetch\u001b[49m\u001b[43m(\u001b[49m\u001b[43mindex\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# may raise StopIteration\u001b[39;00m\n\u001b[32m    790\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m._pin_memory:\n\u001b[32m    791\u001b[39m         data = _utils.pin_memory.pin_memory(data, \u001b[38;5;28mself\u001b[39m._pin_memory_device)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/MSAI/courses/spring_2025/nlp/qt/venv/lib/python3.12/site-packages/torch/utils/data/_utils/fetch.py:52\u001b[39m, in \u001b[36m_MapDatasetFetcher.fetch\u001b[39m\u001b[34m(self, possibly_batched_index)\u001b[39m\n\u001b[32m     50\u001b[39m         data = \u001b[38;5;28mself\u001b[39m.dataset.__getitems__(possibly_batched_index)\n\u001b[32m     51\u001b[39m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m---> \u001b[39m\u001b[32m52\u001b[39m         data = [\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mdataset\u001b[49m\u001b[43m[\u001b[49m\u001b[43midx\u001b[49m\u001b[43m]\u001b[49m \u001b[38;5;28;01mfor\u001b[39;00m idx \u001b[38;5;129;01min\u001b[39;00m possibly_batched_index]\n\u001b[32m     53\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m     54\u001b[39m     data = \u001b[38;5;28mself\u001b[39m.dataset[possibly_batched_index]\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/MSAI/courses/spring_2025/nlp/qt/utils/torch_datasets.py:21\u001b[39m, in \u001b[36m__getitem__\u001b[39m\u001b[34m(self, idx)\u001b[39m\n\u001b[32m     19\u001b[39m start = \u001b[38;5;28mself\u001b[39m.indices[idx]\n\u001b[32m     20\u001b[39m chunk = \u001b[38;5;28mself\u001b[39m.tokens[start : start + \u001b[38;5;28mself\u001b[39m.block_size + \u001b[32m1\u001b[39m]\n\u001b[32m---> \u001b[39m\u001b[32m21\u001b[39m input_tensor = chunk[:-\u001b[32m1\u001b[39m].clone().long()\n\u001b[32m     22\u001b[39m label_tensor = chunk[\u001b[32m1\u001b[39m:].clone().long()\n\u001b[32m     23\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m {\u001b[33m\"\u001b[39m\u001b[33minput_ids\u001b[39m\u001b[33m\"\u001b[39m: input_tensor, \u001b[33m\"\u001b[39m\u001b[33mlabels\u001b[39m\u001b[33m\"\u001b[39m: label_tensor}\n",
      "\u001b[31mKeyboardInterrupt\u001b[39m: "
     ]
    }
   ],
   "source": [
    "pbar = tqdm(enumerate(train_loader), total=len(train_loader))\n",
    "\n",
    "for i, batch in pbar:\n",
    "    torch.sum(batch['input_ids'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input Ids:\n",
      "HTC's Vive Pro headset is available to pre-order for $799\n",
      "\n",
      "We've seen plenty of Beats-focused KIRFs in our time, some better than others. Few, however, play quite so directly on the name as OrigAudio's Beets. For $25, adopters get a set of headphones that bear little direct resemblance to Dr. Dre's audio gear of choice, but are no doubt bound to impress friends -- at least, up until they see a root vegetable\n",
      "\n",
      "Labels:\n",
      "TC's Vive Pro headset is available to pre-order for $799\n",
      "\n",
      "We've seen plenty of Beats-focused KIRFs in our time, some better than others. Few, however, play quite so directly on the name as OrigAudio's Beets. For $25, adopters get a set of headphones that bear little direct resemblance to Dr. Dre's audio gear of choice, but are no doubt bound to impress friends -- at least, up until they see a root vegetable logo\n",
      "\n",
      "\n",
      "\n",
      "Input Ids:\n",
      " logo instead of a lower-case B. Thankfully, there's more to it than just amusing and confusing peers. Every purchase will lead to a donation of canned beets (what else?) to the Second Harvest Food Bank of Orange County. For us, that's reason enough to hope that Beats doesn't put the kibosh on OrigAudio's effort. Besides, we could use some accompaniment for our BeetBox.Q:\n",
      "\n",
      "NullPointerException in getview of custom adapter\n",
      "\n",
      "\n",
      "Labels:\n",
      " instead of a lower-case B. Thankfully, there's more to it than just amusing and confusing peers. Every purchase will lead to a donation of canned beets (what else?) to the Second Harvest Food Bank of Orange County. For us, that's reason enough to hope that Beats doesn't put the kibosh on OrigAudio's effort. Besides, we could use some accompaniment for our BeetBox.Q:\n",
      "\n",
      "NullPointerException in getview of custom adapter\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Input Ids:\n",
      "\n",
      "I'm getting image from bitmap method and trying to populate the listview. But when i call the bitmap function inside getview the nullpointerException error occurs. please help me... \n",
      "here is my view Activity class:\n",
      "public class Viewactivity extends Activity{\n",
      "\n",
      "    TextView tv;\n",
      "    ImageView im;\n",
      "\n",
      "    @Override\n",
      "    protected void onCreate(Bundle savedInstanceState) {\n",
      "     \n",
      "\n",
      "Labels:\n",
      "I'm getting image from bitmap method and trying to populate the listview. But when i call the bitmap function inside getview the nullpointerException error occurs. please help me... \n",
      "here is my view Activity class:\n",
      "public class Viewactivity extends Activity{\n",
      "\n",
      "    TextView tv;\n",
      "    ImageView im;\n",
      "\n",
      "    @Override\n",
      "    protected void onCreate(Bundle savedInstanceState) {\n",
      "      \n",
      "\n",
      "\n",
      "\n",
      "Input Ids:\n",
      "   super.onCreate(savedInstanceState);\n",
      "        setContentView(R.layout.views);\n",
      "\n",
      "        ListView mListView = (ListView)findViewById(R.id.listView);\n",
      "        //array houlds all images\n",
      "        int Images[] = new int[]{\n",
      "         \n",
      "\n",
      "Labels:\n",
      "  super.onCreate(savedInstanceState);\n",
      "        setContentView(R.layout.views);\n",
      "\n",
      "        ListView mListView = (ListView)findViewById(R.id.listView);\n",
      "        //array houlds all images\n",
      "        int Images[] = new int[]{\n",
      "          \n",
      "\n",
      "\n",
      "\n",
      "Input Ids:\n",
      "   R.drawable.confidential,\n",
      "            ...     \n",
      "            };\n",
      "        //array holds all strings to be drawn in the image\n",
      "\n",
      "        CustomList adaptor = new CustomList(this , Images);\n",
      "        mListView.setAdapter(adaptor);\n",
      "\n",
      "\n",
      "\n",
      "Labels:\n",
      "  R.drawable.confidential,\n",
      "            ...     \n",
      "            };\n",
      "        //array holds all strings to be drawn in the image\n",
      "\n",
      "        CustomList adaptor = new CustomList(this , Images);\n",
      "        mListView.setAdapter(adaptor);\n",
      "\n",
      " \n",
      "\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for i in range(5):\n",
    "    batch = train[i]\n",
    "    input_ids = tokenizer.decode(batch[\"input_ids\"])\n",
    "    labels = tokenizer.decode(batch[\"labels\"])\n",
    "    print(f'Input Ids:\\n{input_ids}\\n')\n",
    "    print(f\"Labels:\\n{labels}\")\n",
    "    print('\\n\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Estimating Parameters based on Architecture Hyperparameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "------------Settings-----------------\n",
      "d_model: 2048\n",
      "d_ff: 8192\n",
      "num_layers: 14\n",
      "vocab_size: 50300\n",
      "seq_len: 2048\n",
      "batch_size: 64\n",
      "assumes fp32 params\n",
      "------------Parameters---------------\n",
      "params per decoder layer: 67,108,864\n",
      "--------------------------------------\n",
      "total nonembedding params: 939,524,096\n",
      "total embedding params: 103,014,400\n",
      "--------------------------------------\n",
      "\n",
      "total params: 1,042,538,496\n",
      "\n",
      "----------------Memory----------------\n",
      "\n",
      "memory footprint of model during training: 28 GBs\n",
      "memory footprint of model during inference: 5.0 GBs\n",
      "\n",
      "--------------------------------------\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#settings\n",
    "d_model = 2048\n",
    "num_heads = 16\n",
    "assert d_model % num_heads == 0\n",
    "d_ff = 4*d_model\n",
    "num_layers = 14\n",
    "vocab_size = 50300\n",
    "\n",
    "seq_len = 2048\n",
    "batch_size = 64\n",
    "\n",
    "\n",
    "## parameter calcs\n",
    "decoder_params = 2*4*d_model**2 + 2*d_model*d_ff\n",
    "\n",
    "embedding_params = vocab_size * d_model\n",
    "nonembedding_params = decoder_params * num_layers\n",
    "\n",
    "total_params = embedding_params + nonembedding_params\n",
    "\n",
    "## memory footprint calcs\n",
    "\n",
    "model_footprint = 4 * total_params\n",
    "\n",
    "# inference footprint in GBs\n",
    "inference_footprint = 1.2 * model_footprint // 10**9\n",
    "\n",
    "adam_footprint = 12 * total_params\n",
    "gradients_footprint = 4 * total_params\n",
    "activations_footprint = 2*seq_len*batch_size*d_model*num_layers\n",
    "\n",
    "# training footprint in GBs\n",
    "training_footprint = (model_footprint + \\\n",
    "    adam_footprint + \\\n",
    "    gradients_footprint + \\\n",
    "    activations_footprint) // 10**9\n",
    "\n",
    "model_card_str = f'''\n",
    "------------Settings-----------------\n",
    "d_model: {d_model}\n",
    "d_ff: {d_ff}\n",
    "num_layers: {num_layers}\n",
    "vocab_size: {vocab_size}\n",
    "seq_len: {seq_len}\n",
    "batch_size: {batch_size}\n",
    "assumes fp32 params\n",
    "------------Parameters---------------\n",
    "params per decoder layer: {decoder_params:,}\n",
    "--------------------------------------\n",
    "total nonembedding params: {nonembedding_params:,}\n",
    "total embedding params: {embedding_params:,}\n",
    "--------------------------------------\n",
    "\n",
    "total params: {total_params:,}\n",
    "\n",
    "----------------Memory----------------\n",
    "\n",
    "memory footprint of model during training: {training_footprint} GBs\n",
    "memory footprint of model during inference: {inference_footprint} GBs\n",
    "\n",
    "--------------------------------------\n",
    "\n",
    "'''\n",
    "\n",
    "print(model_card_str)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
