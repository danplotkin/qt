{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/danielplotkin/Documents/MSAI/courses/spring_2025/nlp/qt/venv/lib/python3.12/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "from datasets import load_from_disk\n",
    "from utils.torch_datasets import MiniPileDataset\n",
    "from utils.tokenizer import get_tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1000000/1000000 [05:24<00:00, 3080.02it/s]\n"
     ]
    }
   ],
   "source": [
    "from datasets import load_from_disk\n",
    "from tqdm import tqdm\n",
    "import torch\n",
    "\n",
    "ds = load_from_disk(\"data/tokenized/minipile\")\n",
    "all_ids = []\n",
    "for example in tqdm(ds['train']):\n",
    "    all_ids.extend(example[\"input_ids\"])\n",
    "\n",
    "torch.save(torch.tensor(all_ids, dtype=torch.long), \"data/tokenized/flattened_ids.pt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "train = MiniPileDataset(path='data/tokenized/flattened_ids.pt', block_size=100)\n",
    "tokenizer = get_tokenizer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/danielplotkin/Documents/MSAI/courses/spring_2025/nlp/qt/utils/torch_datasets.py:20: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.detach().clone() or sourceTensor.detach().clone().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  input_tensor = torch.tensor(chunk[:-1].clone(), dtype=torch.long)\n",
      "/Users/danielplotkin/Documents/MSAI/courses/spring_2025/nlp/qt/utils/torch_datasets.py:21: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.detach().clone() or sourceTensor.detach().clone().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  label_tensor = torch.tensor(chunk[1:].clone(), dtype=torch.long)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'input_ids': tensor([   39,  4825,   338, 29237,  1041, 23492,   318,  1695,   284,   662,\n",
       "            12,  2875,   329,   720, 45455,   198,   198,  1135,  1053,  1775,\n",
       "          6088,   286, 40210,    12, 18143,   509,  4663, 42388,   287,   674,\n",
       "           640,    11,   617,  1365,   621,  1854,    13, 20463,    11,  2158,\n",
       "            11,   711,  2407,   523,  3264,   319,   262,  1438,   355,  6913,\n",
       "         21206,   338,  1355,  1039,    13,  1114,   720,  1495,    11,  4344,\n",
       "          1010,   651,   257,   900,   286, 22537,   326,  6842,  1310,  1277,\n",
       "         28204,   284,  1583,    13, 30882,   338,  6597,  7733,   286,  3572,\n",
       "            11,   475,   389,   645,  4719,  5421,   284, 14947,  2460,  1377,\n",
       "           379,  1551,    11,   510,  1566,   484,   766,   257,  6808, 20236]),\n",
       " 'labels': tensor([ 4825,   338, 29237,  1041, 23492,   318,  1695,   284,   662,    12,\n",
       "          2875,   329,   720, 45455,   198,   198,  1135,  1053,  1775,  6088,\n",
       "           286, 40210,    12, 18143,   509,  4663, 42388,   287,   674,   640,\n",
       "            11,   617,  1365,   621,  1854,    13, 20463,    11,  2158,    11,\n",
       "           711,  2407,   523,  3264,   319,   262,  1438,   355,  6913, 21206,\n",
       "           338,  1355,  1039,    13,  1114,   720,  1495,    11,  4344,  1010,\n",
       "           651,   257,   900,   286, 22537,   326,  6842,  1310,  1277, 28204,\n",
       "           284,  1583,    13, 30882,   338,  6597,  7733,   286,  3572,    11,\n",
       "           475,   389,   645,  4719,  5421,   284, 14947,  2460,  1377,   379,\n",
       "          1551,    11,   510,  1566,   484,   766,   257,  6808, 20236, 11112])}"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input Ids:\n",
      "HTC's Vive Pro headset is available to pre-order for $799\n",
      "\n",
      "We've seen plenty of Beats-focused KIRFs in our time, some better than others. Few, however, play quite so directly on the name as OrigAudio's Beets. For $25, adopters get a set of headphones that bear little direct resemblance to Dr. Dre's audio gear of choice, but are no doubt bound to impress friends -- at least, up until they see a root vegetable\n",
      "\n",
      "Labels:\n",
      "TC's Vive Pro headset is available to pre-order for $799\n",
      "\n",
      "We've seen plenty of Beats-focused KIRFs in our time, some better than others. Few, however, play quite so directly on the name as OrigAudio's Beets. For $25, adopters get a set of headphones that bear little direct resemblance to Dr. Dre's audio gear of choice, but are no doubt bound to impress friends -- at least, up until they see a root vegetable logo\n",
      "\n",
      "\n",
      "\n",
      "Input Ids:\n",
      " logo instead of a lower-case B. Thankfully, there's more to it than just amusing and confusing peers. Every purchase will lead to a donation of canned beets (what else?) to the Second Harvest Food Bank of Orange County. For us, that's reason enough to hope that Beats doesn't put the kibosh on OrigAudio's effort. Besides, we could use some accompaniment for our BeetBox.Q:\n",
      "\n",
      "NullPointerException in getview of custom adapter\n",
      "\n",
      "\n",
      "Labels:\n",
      " instead of a lower-case B. Thankfully, there's more to it than just amusing and confusing peers. Every purchase will lead to a donation of canned beets (what else?) to the Second Harvest Food Bank of Orange County. For us, that's reason enough to hope that Beats doesn't put the kibosh on OrigAudio's effort. Besides, we could use some accompaniment for our BeetBox.Q:\n",
      "\n",
      "NullPointerException in getview of custom adapter\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Input Ids:\n",
      "\n",
      "I'm getting image from bitmap method and trying to populate the listview. But when i call the bitmap function inside getview the nullpointerException error occurs. please help me... \n",
      "here is my view Activity class:\n",
      "public class Viewactivity extends Activity{\n",
      "\n",
      "    TextView tv;\n",
      "    ImageView im;\n",
      "\n",
      "    @Override\n",
      "    protected void onCreate(Bundle savedInstanceState) {\n",
      "     \n",
      "\n",
      "Labels:\n",
      "I'm getting image from bitmap method and trying to populate the listview. But when i call the bitmap function inside getview the nullpointerException error occurs. please help me... \n",
      "here is my view Activity class:\n",
      "public class Viewactivity extends Activity{\n",
      "\n",
      "    TextView tv;\n",
      "    ImageView im;\n",
      "\n",
      "    @Override\n",
      "    protected void onCreate(Bundle savedInstanceState) {\n",
      "      \n",
      "\n",
      "\n",
      "\n",
      "Input Ids:\n",
      "   super.onCreate(savedInstanceState);\n",
      "        setContentView(R.layout.views);\n",
      "\n",
      "        ListView mListView = (ListView)findViewById(R.id.listView);\n",
      "        //array houlds all images\n",
      "        int Images[] = new int[]{\n",
      "         \n",
      "\n",
      "Labels:\n",
      "  super.onCreate(savedInstanceState);\n",
      "        setContentView(R.layout.views);\n",
      "\n",
      "        ListView mListView = (ListView)findViewById(R.id.listView);\n",
      "        //array houlds all images\n",
      "        int Images[] = new int[]{\n",
      "          \n",
      "\n",
      "\n",
      "\n",
      "Input Ids:\n",
      "   R.drawable.confidential,\n",
      "            ...     \n",
      "            };\n",
      "        //array holds all strings to be drawn in the image\n",
      "\n",
      "        CustomList adaptor = new CustomList(this , Images);\n",
      "        mListView.setAdapter(adaptor);\n",
      "\n",
      "\n",
      "\n",
      "Labels:\n",
      "  R.drawable.confidential,\n",
      "            ...     \n",
      "            };\n",
      "        //array holds all strings to be drawn in the image\n",
      "\n",
      "        CustomList adaptor = new CustomList(this , Images);\n",
      "        mListView.setAdapter(adaptor);\n",
      "\n",
      " \n",
      "\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for i in range(5):\n",
    "    batch = train[i]\n",
    "    input_ids = tokenizer.decode(batch[\"input_ids\"])\n",
    "    labels = tokenizer.decode(batch[\"labels\"])\n",
    "    print(f'Input Ids:\\n{input_ids}\\n')\n",
    "    print(f\"Labels:\\n{labels}\")\n",
    "    print('\\n\\n')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
