{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## qt Training Playground"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                 \r"
     ]
    }
   ],
   "source": [
    "from datasets import load_from_disk\n",
    "from utils.torch_datasets import MiniPileDataset, ExampleCorpusDataset, RedditCommentsDataset, PretrainedCorpaDataset\n",
    "from utils.tokenizer import get_tokenizer\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "from tqdm import tqdm\n",
    "import torch\n",
    "\n",
    "tokenizer = get_tokenizer()\n",
    "dataset = PretrainedCorpaDataset('train', 100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input ids: Then you need to figure out a way to quote the quotation marks. But then you might as well quote spaces too... which is what we do (as %20).\n",
      "\n",
      "Ultimately there are many solutions to this problem, but the one the engineers behind this system decided to use was the %-encoding system.[deleted]stab me once shame on you, stab me twice shame on me[removed]**Please read this entire message before taking action.**\n",
      "\n",
      "\n",
      "Thanks for\n",
      "Label ids:  you need to figure out a way to quote the quotation marks. But then you might as well quote spaces too... which is what we do (as %20).\n",
      "\n",
      "Ultimately there are many solutions to this problem, but the one the engineers behind this system decided to use was the %-encoding system.[deleted]stab me once shame on you, stab me twice shame on me[removed]**Please read this entire message before taking action.**\n",
      "\n",
      "\n",
      "Thanks for participating\n",
      "**************************************************\n",
      "Input ids: :185) ~[spring-aop-5.0.7.RELEASE.jar:5.0.7.RELEASE]\n",
      "    at org.springframework.aop.interceptor.ExposeInvocationInterceptor.invoke(ExposeInvocationInterceptor.java:92) ~[spring-aop-5.0.7.RELEASE.jar:5.0.7.RELEASE]\n",
      "    at org.\n",
      "Label ids: 185) ~[spring-aop-5.0.7.RELEASE.jar:5.0.7.RELEASE]\n",
      "    at org.springframework.aop.interceptor.ExposeInvocationInterceptor.invoke(ExposeInvocationInterceptor.java:92) ~[spring-aop-5.0.7.RELEASE.jar:5.0.7.RELEASE]\n",
      "    at org.spring\n",
      "**************************************************\n",
      "Input ids:  completely integrated with the civilian population, and you have no way of knowing who is in ISIS and who isn't.  Thus any sort of broad action against them **will** end up killing tons and tons of innocent civilians.  And that's considered to be unacceptable, both because of the civilian deaths directly, *and* because killing civilians will just drive more people to join ISIS.\n",
      "\n",
      "If you want an analogy, it's kind of like asking why police don't just arrest all gang members\n",
      "Label ids:  integrated with the civilian population, and you have no way of knowing who is in ISIS and who isn't.  Thus any sort of broad action against them **will** end up killing tons and tons of innocent civilians.  And that's considered to be unacceptable, both because of the civilian deaths directly, *and* because killing civilians will just drive more people to join ISIS.\n",
      "\n",
      "If you want an analogy, it's kind of like asking why police don't just arrest all gang members:\n",
      "**************************************************\n",
      "Input ids:  Unless required by applicable law or agreed to in writing, software\n",
      " * distributed under the License is distributed on an \"AS IS\" BASIS, WITHOUT\n",
      " * WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the\n",
      " * License for the specific language governing permissions and limitations under\n",
      " * the License.\n",
      " */\n",
      "\n",
      "/**\n",
      " * The \"adunits\" collection of methods.\n",
      " * Typical usage is:\n",
      " *  <code>\n",
      " *   $\n",
      "Label ids:  required by applicable law or agreed to in writing, software\n",
      " * distributed under the License is distributed on an \"AS IS\" BASIS, WITHOUT\n",
      " * WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the\n",
      " * License for the specific language governing permissions and limitations under\n",
      " * the License.\n",
      " */\n",
      "\n",
      "/**\n",
      " * The \"adunits\" collection of methods.\n",
      " * Typical usage is:\n",
      " *  <code>\n",
      " *   $ads\n",
      "**************************************************\n",
      "Input ids: : 16px;\n",
      "            }\n",
      "            #container2 #span1{\n",
      "                color: #5D8AA8;\n",
      "                margin-right: 5px;\n",
      "            }\n",
      "  \n",
      "Label ids:  16px;\n",
      "            }\n",
      "            #container2 #span1{\n",
      "                color: #5D8AA8;\n",
      "                margin-right: 5px;\n",
      "            }\n",
      "   \n",
      "**************************************************\n",
      "Input ids: 're half right. \n",
      "\n",
      "They are having their views amplified but not quite in that way.\n",
      "\n",
      "There's a tendency to focus on the wild, outlandish aspects of certain communities and if that's the only interaction with that community you really have, it's easy to see the entire community through that lens.\n",
      "\n",
      "With the Hawaiin shirt assholes OP is talking about, they are 100% right in that the people who do this are generally tipping their hat in a specific direction but it\n",
      "Label ids:  half right. \n",
      "\n",
      "They are having their views amplified but not quite in that way.\n",
      "\n",
      "There's a tendency to focus on the wild, outlandish aspects of certain communities and if that's the only interaction with that community you really have, it's easy to see the entire community through that lens.\n",
      "\n",
      "With the Hawaiin shirt assholes OP is talking about, they are 100% right in that the people who do this are generally tipping their hat in a specific direction but it's\n",
      "**************************************************\n",
      "Input ids:  punishment on (someone/something).\" I have never heard of someone's childhood bully finding out about their success in their career/social life/whatever and having that feel as punishing as being humiliated or hurt. Revenge is the best revenge. Living well is not.That’s a good reason to murder someoneProbably was on the news but he missed it that day when he was on the computer donating $250 by maxing our his credit cards.[removed]Just because you don't remember\n",
      "Label ids:  on (someone/something).\" I have never heard of someone's childhood bully finding out about their success in their career/social life/whatever and having that feel as punishing as being humiliated or hurt. Revenge is the best revenge. Living well is not.That’s a good reason to murder someoneProbably was on the news but he missed it that day when he was on the computer donating $250 by maxing our his credit cards.[removed]Just because you don't remember something\n",
      "**************************************************\n",
      "Input ids:  and upgrade your GPU if you're only focused on gaming.\n",
      "\n",
      "A Ryzen 5 1600af is cheaper but won't really compromise gaming performance and the money you get from downgrading means you can get a significantly better GPU. Potentially even a 2070 or an RX 5700 XT depending on what you like /what you can find deals for.\n",
      "\n",
      "Of course for 3d rendering or video editing the extra CPU horsepower might make the 3600 worth it (I have the 3600 in my\n",
      "Label ids:  upgrade your GPU if you're only focused on gaming.\n",
      "\n",
      "A Ryzen 5 1600af is cheaper but won't really compromise gaming performance and the money you get from downgrading means you can get a significantly better GPU. Potentially even a 2070 or an RX 5700 XT depending on what you like /what you can find deals for.\n",
      "\n",
      "Of course for 3d rendering or video editing the extra CPU horsepower might make the 3600 worth it (I have the 3600 in my build\n",
      "**************************************************\n",
      "Input ids:  Even the Dead Sea Scrolls are at least 255 years older than the Nicene council so it has been proven that the book of Isaiah was very carefully and faithfully copied.  Therefore, there is no reason to suspect the new testament books weren't copied just as well.Churches had significantly more power and money than they do now, though they are quite rich still.  Back in the day having a massive place of worship, be it Temple, Synagogal, Church/Cathedral, etc.\n",
      "Label ids:  the Dead Sea Scrolls are at least 255 years older than the Nicene council so it has been proven that the book of Isaiah was very carefully and faithfully copied.  Therefore, there is no reason to suspect the new testament books weren't copied just as well.Churches had significantly more power and money than they do now, though they are quite rich still.  Back in the day having a massive place of worship, be it Temple, Synagogal, Church/Cathedral, etc. was\n",
      "**************************************************\n",
      "Input ids:  high quality NVMe M.2 drive.\n",
      "\n",
      "With the 53 CAD we saved, we can now take that money and get you a much better built B550 motherboard with more features. Additionally, your motherboard doesn't have an internal Type-C header to work with the front Type-C port on your case, so a different motherboard is needed.\n",
      "\n",
      "Adding that 53 CAD to the motherboard budget gives you a motherboard budget of around 210 CAD. For that budget, there are two motherboards\n",
      "Label ids:  quality NVMe M.2 drive.\n",
      "\n",
      "With the 53 CAD we saved, we can now take that money and get you a much better built B550 motherboard with more features. Additionally, your motherboard doesn't have an internal Type-C header to work with the front Type-C port on your case, so a different motherboard is needed.\n",
      "\n",
      "Adding that 53 CAD to the motherboard budget gives you a motherboard budget of around 210 CAD. For that budget, there are two motherboards to\n",
      "**************************************************\n",
      "Input ids: dairy milk, shelled seed, soaps and lotions have continued to skyrocket against the backdrop of the new hemp research provision in the Farm Bill, and increasing grassroots pressure to allow hemp to be grown domestically on a commercial scale once again for U.S. processors and manufacturers. The HIA has also reviewed sales of clothing, auto parts, building materials and various other products, and estimates the total retail value of hemp products sold in the U.S. in 2014 to be at least $\n",
      "Label ids: airy milk, shelled seed, soaps and lotions have continued to skyrocket against the backdrop of the new hemp research provision in the Farm Bill, and increasing grassroots pressure to allow hemp to be grown domestically on a commercial scale once again for U.S. processors and manufacturers. The HIA has also reviewed sales of clothing, auto parts, building materials and various other products, and estimates the total retail value of hemp products sold in the U.S. in 2014 to be at least $620\n",
      "**************************************************\n",
      "Input ids:  World Cup.Marvel has tried more, and they never really tried hard. Look, I get that Marvel wins a numbers game hands down-but DC hasn't been playing the game for a long time, marvel has. Marvel also has a better long term plan. But the movies they have been making are shallow. There's nothing to them. And since every release has been like that, there's no evidence that they're going to change. I'll concede to your christopher Nolan point; it\n",
      "Label ids:  Cup.Marvel has tried more, and they never really tried hard. Look, I get that Marvel wins a numbers game hands down-but DC hasn't been playing the game for a long time, marvel has. Marvel also has a better long term plan. But the movies they have been making are shallow. There's nothing to them. And since every release has been like that, there's no evidence that they're going to change. I'll concede to your christopher Nolan point; it's\n",
      "**************************************************\n",
      "Input ids:  and Ruby that would prevent\n",
      "ActiveRecord from ever working? Why would it have to be \"built in\"?\n",
      "\n",
      "Edit: This link <http://www.avibryant.com/2008/06/maglev-recap.html> from the\n",
      "other discussion thread gave me more information. Still, if Maglev is Ruby (or\n",
      "Ruby+, maybe?) and ActiveRecord is written in Ruby, then bugs and minor\n",
      "incompatibilities aside, shouldn't ActiveRecord\n",
      "Label ids:  Ruby that would prevent\n",
      "ActiveRecord from ever working? Why would it have to be \"built in\"?\n",
      "\n",
      "Edit: This link <http://www.avibryant.com/2008/06/maglev-recap.html> from the\n",
      "other discussion thread gave me more information. Still, if Maglev is Ruby (or\n",
      "Ruby+, maybe?) and ActiveRecord is written in Ruby, then bugs and minor\n",
      "incompatibilities aside, shouldn't ActiveRecord just\n",
      "**************************************************\n",
      "Input ids:  gene ([@B14]) and the human IgG1 constant domains were created as follows. External primers encoding the 5′ portion and the 3′ portion of mOX40 were used to amplify the extracellular portion of mOX40 (from mouse spleen cDNA). Each primer contained appropriate restriction sites for subcloning into the human IgG1 expression vector ([@B13]), together with a 3′ splice donor site within the 3′ primer to correctly splice to the\n",
      "Label ids:  ([@B14]) and the human IgG1 constant domains were created as follows. External primers encoding the 5′ portion and the 3′ portion of mOX40 were used to amplify the extracellular portion of mOX40 (from mouse spleen cDNA). Each primer contained appropriate restriction sites for subcloning into the human IgG1 expression vector ([@B13]), together with a 3′ splice donor site within the 3′ primer to correctly splice to the human\n",
      "**************************************************\n",
      "Input ids: [deleted]Okay, still too high, to me it sounds like a beefier cooler is something you might want to look into, undervolting the CPU could also be something to look into.Thats been my biggest worry since thats not a part I would be able to test myself and is one of the more expensive parts to replace. Should be under warranty though, on the bright side. The repair shop said they didn't see anything wrong there but thats not definitive, so I think\n",
      "Label ids: deleted]Okay, still too high, to me it sounds like a beefier cooler is something you might want to look into, undervolting the CPU could also be something to look into.Thats been my biggest worry since thats not a part I would be able to test myself and is one of the more expensive parts to replace. Should be under warranty though, on the bright side. The repair shop said they didn't see anything wrong there but thats not definitive, so I think its\n",
      "**************************************************\n",
      "Input ids:  lilv_new_uri(world, LILV_URI_OUTPUT_PORT);\n",
      "\turis.port = lilv_new_uri(world, LILV_URI_PORT);\n",
      "\n",
      "\turis.lv2_AudioPort = lilv_new_uri(world, LV2_CORE__AudioPort);\n",
      "\turis.lv2_CVPort = lilv_new_uri(world, LV2_CORE\n",
      "Label ids: v_new_uri(world, LILV_URI_OUTPUT_PORT);\n",
      "\turis.port = lilv_new_uri(world, LILV_URI_PORT);\n",
      "\n",
      "\turis.lv2_AudioPort = lilv_new_uri(world, LV2_CORE__AudioPort);\n",
      "\turis.lv2_CVPort = lilv_new_uri(world, LV2_CORE__\n",
      "**************************************************\n"
     ]
    }
   ],
   "source": [
    "dl_shuffle = DataLoader(dataset ,batch_size=16, shuffle=True)\n",
    "for i, batch in enumerate(dl_shuffle):\n",
    "    x_batch, y_batch = batch\n",
    "    for x, y in zip(x_batch, y_batch):\n",
    "        print(f\"Input ids: {tokenizer.decode(x)}\")\n",
    "        print(f\"Label ids: {tokenizer.decode(y)}\")\n",
    "        print('*'*50)\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The cat sat on the mat.  \n",
      " ->  cat sat on the mat.  \n",
      "The\n",
      "\n",
      "\n",
      "\n",
      " cat sat on the mat.  \n",
      "The ->  sat on the mat.  \n",
      "The dog\n",
      "\n",
      "\n",
      "\n",
      " sat on the mat.  \n",
      "The dog ->  on the mat.  \n",
      "The dog bark\n",
      "\n",
      "\n",
      "\n",
      " on the mat.  \n",
      "The dog bark ->  the mat.  \n",
      "The dog barked\n",
      "\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "ds = ExampleCorpusDataset(10, 1)\n",
    "for i in range(4):\n",
    "    ex_x, ex_y = ds[i]\n",
    "    print(tokenizer.decode(ex_x), tokenizer.decode(ex_y), sep=' -> ')\n",
    "    print('\\n\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(48024)"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ds.tokens.max()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "HTC's Vive Pro headset is available to pre-order for $799\n",
      "\n",
      "We've seen plenty of Beats-focused KIRFs in our time, some better than others. Few, however, play quite so directly on the name as OrigAudio's Beets. For $25, adopters get a set of headphones that bear little direct resemblance to Dr. Dre's audio gear of choice, but are no doubt bound to impress friends -- at least, up until they see a root vegetable -> TC's Vive Pro headset is available to pre-order for $799\n",
      "\n",
      "We've seen plenty of Beats-focused KIRFs in our time, some better than others. Few, however, play quite so directly on the name as OrigAudio's Beets. For $25, adopters get a set of headphones that bear little direct resemblance to Dr. Dre's audio gear of choice, but are no doubt bound to impress friends -- at least, up until they see a root vegetable logo\n",
      "**************************************************\n",
      " logo instead of a lower-case B. Thankfully, there's more to it than just amusing and confusing peers. Every purchase will lead to a donation of canned beets (what else?) to the Second Harvest Food Bank of Orange County. For us, that's reason enough to hope that Beats doesn't put the kibosh on OrigAudio's effort. Besides, we could use some accompaniment for our BeetBox.Q:\n",
      "\n",
      "NullPointerException in getview of custom adapter\n",
      " ->  instead of a lower-case B. Thankfully, there's more to it than just amusing and confusing peers. Every purchase will lead to a donation of canned beets (what else?) to the Second Harvest Food Bank of Orange County. For us, that's reason enough to hope that Beats doesn't put the kibosh on OrigAudio's effort. Besides, we could use some accompaniment for our BeetBox.Q:\n",
      "\n",
      "NullPointerException in getview of custom adapter\n",
      "\n",
      "\n",
      "**************************************************\n",
      "\n",
      "I'm getting image from bitmap method and trying to populate the listview. But when i call the bitmap function inside getview the nullpointerException error occurs. please help me... \n",
      "here is my view Activity class:\n",
      "public class Viewactivity extends Activity{\n",
      "\n",
      "    TextView tv;\n",
      "    ImageView im;\n",
      "\n",
      "    @Override\n",
      "    protected void onCreate(Bundle savedInstanceState) {\n",
      "      -> I'm getting image from bitmap method and trying to populate the listview. But when i call the bitmap function inside getview the nullpointerException error occurs. please help me... \n",
      "here is my view Activity class:\n",
      "public class Viewactivity extends Activity{\n",
      "\n",
      "    TextView tv;\n",
      "    ImageView im;\n",
      "\n",
      "    @Override\n",
      "    protected void onCreate(Bundle savedInstanceState) {\n",
      "      \n",
      "**************************************************\n",
      "   super.onCreate(savedInstanceState);\n",
      "        setContentView(R.layout.views);\n",
      "\n",
      "        ListView mListView = (ListView)findViewById(R.id.listView);\n",
      "        //array houlds all images\n",
      "        int Images[] = new int[]{\n",
      "          ->   super.onCreate(savedInstanceState);\n",
      "        setContentView(R.layout.views);\n",
      "\n",
      "        ListView mListView = (ListView)findViewById(R.id.listView);\n",
      "        //array houlds all images\n",
      "        int Images[] = new int[]{\n",
      "          \n",
      "**************************************************\n"
     ]
    }
   ],
   "source": [
    "train_ds = MiniPileDataset(split='train', block_size=100)\n",
    "tokenizer = get_tokenizer()\n",
    "for i in range(4):\n",
    "    ex_x, ex_y = train_ds[i]\n",
    "    print(tokenizer.decode(ex_x), tokenizer.decode(ex_y), sep=' -> ')\n",
    "    print('*'*50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([   39,  4825,   338, 29237,  1041, 23492,   318,  1695,   284,   662,\n",
       "            12,  2875,   329,   720, 45455,   198,   198,  1135,  1053,  1775,\n",
       "          6088,   286, 40210,    12, 18143,   509,  4663, 42388,   287,   674,\n",
       "           640,    11,   617,  1365,   621,  1854,    13, 20463,    11,  2158,\n",
       "            11,   711,  2407,   523,  3264,   319,   262,  1438,   355,  6913,\n",
       "         21206,   338,  1355,  1039,    13,  1114,   720,  1495,    11,  4344,\n",
       "          1010,   651,   257,   900,   286, 22537,   326,  6842,  1310,  1277,\n",
       "         28204,   284,  1583,    13, 30882,   338,  6597,  7733,   286,  3572,\n",
       "            11,   475,   389,   645,  4719,  5421,   284, 14947,  2460,  1377,\n",
       "           379,  1551,    11,   510,  1566,   484,   766,   257,  6808, 20236]),\n",
       " tensor([ 4825,   338, 29237,  1041, 23492,   318,  1695,   284,   662,    12,\n",
       "          2875,   329,   720, 45455,   198,   198,  1135,  1053,  1775,  6088,\n",
       "           286, 40210,    12, 18143,   509,  4663, 42388,   287,   674,   640,\n",
       "            11,   617,  1365,   621,  1854,    13, 20463,    11,  2158,    11,\n",
       "           711,  2407,   523,  3264,   319,   262,  1438,   355,  6913, 21206,\n",
       "           338,  1355,  1039,    13,  1114,   720,  1495,    11,  4344,  1010,\n",
       "           651,   257,   900,   286, 22537,   326,  6842,  1310,  1277, 28204,\n",
       "           284,  1583,    13, 30882,   338,  6597,  7733,   286,  3572,    11,\n",
       "           475,   389,   645,  4719,  5421,   284, 14947,  2460,  1377,   379,\n",
       "          1551,    11,   510,  1566,   484,   766,   257,  6808, 20236, 11112]))"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_ds[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Estimating Parameters based on Architecture Hyperparameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "------------Settings-----------------\n",
      "d_model: 2048\n",
      "d_ff: 8192\n",
      "num_layers: 1\n",
      "vocab_size: 50300\n",
      "seq_len: 2048\n",
      "batch_size: 64\n",
      "assumes fp32 params\n",
      "------------Parameters---------------\n",
      "params per decoder layer: 67,108,864\n",
      "--------------------------------------\n",
      "total nonembedding params: 67,108,864\n",
      "total embedding params: 103,014,400\n",
      "--------------------------------------\n",
      "\n",
      "total params: 170,123,264\n",
      "\n",
      "----------------Memory----------------\n",
      "\n",
      "memory footprint of model during training: 3 GBs\n",
      "memory footprint of model during inference: 0.0 GBs\n",
      "\n",
      "--------------------------------------\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# settings\n",
    "d_model = 2048\n",
    "num_heads = 16\n",
    "assert d_model % num_heads == 0\n",
    "d_ff = 4*d_model\n",
    "num_layers = 14\n",
    "# num_layers = 1 # NOTE for testing\n",
    "vocab_size = 50300\n",
    "\n",
    "seq_len = 2048\n",
    "batch_size = 64\n",
    "\n",
    "\n",
    "## parameter calcs\n",
    "decoder_params = 2*4*d_model**2 + 2*d_model*d_ff\n",
    "\n",
    "embedding_params = vocab_size * d_model\n",
    "nonembedding_params = decoder_params * num_layers\n",
    "\n",
    "total_params = embedding_params + nonembedding_params\n",
    "\n",
    "## memory footprint calcs\n",
    "\n",
    "model_footprint = 4 * total_params\n",
    "\n",
    "# inference footprint in GBs\n",
    "inference_footprint = 1.2 * model_footprint // 10**9\n",
    "\n",
    "adam_footprint = 12 * total_params\n",
    "gradients_footprint = 4 * total_params\n",
    "activations_footprint = 2*seq_len*batch_size*d_model*num_layers\n",
    "\n",
    "# training footprint in GBs\n",
    "training_footprint = (model_footprint + \\\n",
    "    adam_footprint + \\\n",
    "    gradients_footprint + \\\n",
    "    activations_footprint) // 10**9\n",
    "\n",
    "model_card_str = f'''\n",
    "------------Settings-----------------\n",
    "d_model: {d_model}\n",
    "d_ff: {d_ff}\n",
    "num_layers: {num_layers}\n",
    "vocab_size: {vocab_size}\n",
    "seq_len: {seq_len}\n",
    "batch_size: {batch_size}\n",
    "assumes fp32 params\n",
    "------------Parameters---------------\n",
    "params per decoder layer: {decoder_params:,}\n",
    "--------------------------------------\n",
    "total nonembedding params: {nonembedding_params:,}\n",
    "total embedding params: {embedding_params:,}\n",
    "--------------------------------------\n",
    "\n",
    "total params: {total_params:,}\n",
    "\n",
    "----------------Memory----------------\n",
    "\n",
    "memory footprint of model during training: {training_footprint} GBs\n",
    "memory footprint of model during inference: {inference_footprint} GBs\n",
    "\n",
    "--------------------------------------\n",
    "\n",
    "'''\n",
    "\n",
    "print(model_card_str)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Trial Training Run"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/48 [00:00<?, ?it/s]\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "type object 'GPT2TokenizerFast' has no attribute 'pad_token_id'",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mAttributeError\u001b[39m                            Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[1]\u001b[39m\u001b[32m, line 63\u001b[39m\n\u001b[32m     52\u001b[39m \u001b[38;5;66;03m# ## pretrain\u001b[39;00m\n\u001b[32m     53\u001b[39m trainer = Trainer(\n\u001b[32m     54\u001b[39m     model = qt,\n\u001b[32m     55\u001b[39m     train_loader=train_loader,\n\u001b[32m   (...)\u001b[39m\u001b[32m     60\u001b[39m     device=device\n\u001b[32m     61\u001b[39m )\n\u001b[32m---> \u001b[39m\u001b[32m63\u001b[39m \u001b[43mtrainer\u001b[49m\u001b[43m.\u001b[49m\u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/vault/gradschool/qt/utils/training.py:113\u001b[39m, in \u001b[36mTrainer.train\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m    111\u001b[39m inputs, targets = inputs.to(\u001b[38;5;28mself\u001b[39m.device), targets.to(\u001b[38;5;28mself\u001b[39m.device)\n\u001b[32m    112\u001b[39m \u001b[38;5;28mself\u001b[39m.optimizer.zero_grad()\n\u001b[32m--> \u001b[39m\u001b[32m113\u001b[39m logits = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\u001b[43minputs\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;66;03m# batch, seq_len, vocab_size\u001b[39;00m\n\u001b[32m    114\u001b[39m \u001b[38;5;66;03m# Shift for next-token prediction\u001b[39;00m\n\u001b[32m    115\u001b[39m loss = \u001b[38;5;28mself\u001b[39m.criterion(logits, targets)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/vault/gradschool/dlvenv/lib/python3.11/site-packages/torch/nn/modules/module.py:1739\u001b[39m, in \u001b[36mModule._wrapped_call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1737\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._compiled_call_impl(*args, **kwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[32m   1738\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1739\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/vault/gradschool/dlvenv/lib/python3.11/site-packages/torch/nn/modules/module.py:1750\u001b[39m, in \u001b[36mModule._call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1745\u001b[39m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[32m   1746\u001b[39m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[32m   1747\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m._backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_pre_hooks\n\u001b[32m   1748\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[32m   1749\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[32m-> \u001b[39m\u001b[32m1750\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1752\u001b[39m result = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1753\u001b[39m called_always_called_hooks = \u001b[38;5;28mset\u001b[39m()\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/vault/gradschool/qt/utils/transformer/model.py:83\u001b[39m, in \u001b[36mQT.forward\u001b[39m\u001b[34m(self, tgt)\u001b[39m\n\u001b[32m     81\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, tgt: torch.Tensor) -> torch.Tensor:\n\u001b[32m     82\u001b[39m \u001b[38;5;250m    \u001b[39m\u001b[33;03m\"\"\"Decoder-only transformer forward pass\"\"\"\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m83\u001b[39m     tgt_mask = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mgenerate_mask\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtgt\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     84\u001b[39m     \u001b[38;5;66;03m# tgt_embedded = self.dropout(self.positional_encoding(self.decoder_embedding(tgt)))\u001b[39;00m\n\u001b[32m     85\u001b[39m     \u001b[38;5;66;03m# NOTE no positional encodings, using ALiBi \u001b[39;00m\n\u001b[32m     86\u001b[39m     tgt_embedded = \u001b[38;5;28mself\u001b[39m.dropout(\u001b[38;5;28mself\u001b[39m.decoder_embedding(tgt))\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/vault/gradschool/qt/utils/transformer/model.py:75\u001b[39m, in \u001b[36mQT.generate_mask\u001b[39m\u001b[34m(self, tgt)\u001b[39m\n\u001b[32m     73\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mgenerate_mask\u001b[39m(\u001b[38;5;28mself\u001b[39m, tgt: torch.Tensor) -> torch.Tensor:\n\u001b[32m     74\u001b[39m \u001b[38;5;250m    \u001b[39m\u001b[33;03m\"\"\"Generate casual and padding mask\"\"\"\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m75\u001b[39m     tgt_mask = (tgt != \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mtokenizer\u001b[49m\u001b[43m.\u001b[49m\u001b[43mpad_token_id\u001b[49m).unsqueeze(\u001b[32m1\u001b[39m).unsqueeze(\u001b[32m3\u001b[39m)\n\u001b[32m     76\u001b[39m     seq_length = tgt.size(\u001b[32m1\u001b[39m)\n\u001b[32m     77\u001b[39m     nopeak_mask = (\u001b[32m1\u001b[39m - torch.triu(torch.ones(\u001b[32m1\u001b[39m, seq_length, seq_length), diagonal=\u001b[32m1\u001b[39m)).bool().to(tgt.device)\n",
      "\u001b[31mAttributeError\u001b[39m: type object 'GPT2TokenizerFast' has no attribute 'pad_token_id'"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import logging\n",
    "import torch\n",
    "from torchinfo import summary # TODO add this to requirements.txt\n",
    "from torcheval.metrics.text import Perplexity\n",
    "from utils.configs import load_configs\n",
    "from transformers import GPT2TokenizerFast\n",
    "from utils.torch_datasets import MiniPileDataset\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "\n",
    "from utils.transformer.model import QT\n",
    "from utils.training import Trainer\n",
    "\n",
    "## load configs, logger, and device\n",
    "config = load_configs()\n",
    "# logs saves to training.log in harm2d directory\n",
    "logger = logging.getLogger(__name__)\n",
    "logging.basicConfig(\n",
    "    filename=config['training'].logging_dir,\n",
    "    filemode='w',\n",
    "    level=logging.DEBUG,\n",
    "    format='%(asctime)s - %(levelname)s - %(message)s'\n",
    ")\n",
    "# get device\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "## get datasets\n",
    "train = MiniPileDataset(\n",
    "    path='data/tokenized/validation_tokens.pt', \n",
    "    block_size=config['transformer'].max_seq_length\n",
    ")\n",
    "valid = MiniPileDataset(\n",
    "    path='data/tokenized/test_tokens.pt', \n",
    "    block_size=config['transformer'].max_seq_length\n",
    ")\n",
    "\n",
    "## get dataloaders\n",
    "train_loader = DataLoader(train, batch_size=config['training'].batch_size, shuffle=True)\n",
    "valid_loader = DataLoader(valid, batch_size=config['training'].batch_size, shuffle=False)\n",
    "\n",
    "## get model\n",
    "qt = QT(\n",
    "    config=config['transformer'],\n",
    "    tokenizer = GPT2TokenizerFast,\n",
    "    device = device \n",
    ")\n",
    "\n",
    "model_card_str = summary(qt)\n",
    "logging.info('\\n' + str(model_card_str))\n",
    "logging.info(config)\n",
    "\n",
    "# ## pretrain\n",
    "trainer = Trainer(\n",
    "    model = qt,\n",
    "    train_loader=train_loader,\n",
    "    val_loader=valid_loader,\n",
    "    config=config['training'],\n",
    "    criterion = torch.nn.CrossEntropyLoss(),\n",
    "    metric = Perplexity(),\n",
    "    device=device\n",
    ")\n",
    "\n",
    "trainer.train()\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
