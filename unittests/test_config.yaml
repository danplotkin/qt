training:
  model_name: "qt-pretrain"
  learning_rate: 1e-4
  batch_size: 32
  epochs: 10
  weight_decay: 0.01
  output_dir: "./experiments"
  early_stopping: true
  early_stopping_patience: 5
  early_stopping_min_delta: 0.001
  early_stopping_mode: "min"
  restore_best_model: true


# transformer:
#   tgt_vocab_size: 100
#   d_model: 100
#   num_heads: 2
#   num_layers: 1
#   d_ff: 10
#   max_seq_length: 100
#   dropout: 0.1

transformer:
  tgt_vocab_size: 50257
  d_model: 768
  num_heads: 1
  num_layers: 1
  d_ff: 10
  max_seq_length: 50
  dropout: 0.1