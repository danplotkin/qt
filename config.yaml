# configs for training transformer
training:
  learning_rate: 1e-4
  batch_size: 32
  epochs: 10
  weight_decay: 0.01
  output_dir: "./experiments"
  early_stopping: true
  early_stopping_patience: 5
  early_stopping_min_delta: 0.001
  early_stopping_mode: "min"
  restore_best_model: true
  logging_dir: "./logs"
  s3_bucket: "dp-jh-nb-bucket"
  s3_prefix: "QT"

# qt configs
transformer:
  model_name: "qt-finetuned"
  tgt_vocab_size: 50257
  d_model: 736
  num_heads: 8
  num_layers: 8
  d_ff: 1024
  max_seq_length: 512
  dropout: 0.1
